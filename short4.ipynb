{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Правдоподобие в физике высоких энергий\n",
    "\n",
    "В прошлых лекциях мы установили, что для $N$ независимых наблюдений $x_1, \\ldots, x_N$, мы можем записать функцию правдоподобия\n",
    "\n",
    "$$\n",
    "L(\\vec{\\theta}|\\mathbf{X}) = \\prod_{n=1}^{N} f(x_n | \\vec{\\theta})\n",
    "$$\n",
    "\n",
    "На практике, в наших экспериментах число $N$ является также случайной величиной, а следовательно - нам нужно модифицировать функцию правдоподобия, чтобы учесть этот факт\n",
    "\n",
    "$$\n",
    "L(\\vec{\\theta}|\\mathbf{X}) = f(N|\\vec{\\theta}) \\prod_{n=1}^{N} f(x_n | \\vec{\\theta})\n",
    "$$\n",
    "\n",
    "Чаще всего $f(N|\\vec{\\theta})$ - это распределение Пуассона, что в итоге нам дает\n",
    "\n",
    "$$\n",
    "L(\\vec{\\theta}|\\mathbf{X}) = \\frac{\\lambda^N}{N!} e^{-\\lambda} \\prod_{n=1}^{N} f(x_n | \\vec{\\theta})\n",
    "$$\n",
    "\n",
    "Обычно, мы будем выделять фоновые события (background) и сигнальные события (signal). Для каждого из них будет свое распределение $f_b$ и $f_s$. Полное распределение $f$ является смесью этих распределений\n",
    "\n",
    "$$\n",
    "L(\\vec{\\theta}|\\mathbf{X}) = \\frac{\\lambda^N}{N!} e^{-\\lambda(\\vec{\\theta})} \n",
    "\\prod_{n=1}^{N} \\left(\n",
    "c_s f_s(x_n | \\vec{\\theta}) + c_b f_b(x_n | \\vec{\\theta}) \n",
    "\\right)\n",
    "$$\n",
    "\n",
    "где $c_s + c_b = 1$, $s$ и $b$ - среднее ожидаемое количество сигнальных и фоновых событий. Значения $c_s$ и $c_s$ довольно легко найти - это просто доля соответствующих событий к полному числу предполагаемых событий (вероятность того, что это фон или сигнал по сути)\n",
    "\n",
    "$$\n",
    "c_s = \\frac{s}{s + b} \n",
    "\\\\\n",
    "c_b = \\frac{b}{s + b}\n",
    "\\\\\n",
    "\\lambda = s + b\n",
    "$$\n",
    "\n",
    "В итоге\n",
    "\n",
    "$$\n",
    "L(\\vec{\\theta}|\\mathbf{X}) \n",
    "= \n",
    "\\frac{ 1 }{ N!} \n",
    "e^{-(s + b)} \n",
    "\\prod_{n=1}^{N} \\left(\n",
    "s f_s(x_n | \\vec{\\theta}) + b f_b(x_n | \\vec{\\theta}) \n",
    "\\right)\n",
    "$$\n",
    "\n",
    "или\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\ln L = -(s + b) - N! + \\sum_n \\ln\\left(\n",
    "s f_s(x_n | \\vec{\\theta}) + b f_b(x_n | \\vec{\\theta}) \n",
    "\\right)\n",
    "$$\n",
    "\n",
    "В самом общем случае, $s$ и $b$ - это тоже функции вектора параметров $\\vec{\\theta}$.\n",
    "\n",
    "Часто вводят $s = \\mu s_0$, где $\\mu$ - это сила сигнала (0 - нет сигнала, 1 - есть), а $s_0$ - предсказываемое теорией значение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Гистрограммы\n",
    "\n",
    "Иногда наши данные представлены исключительно в виде гистограммы. Что нам делать в этом случае?\n",
    "\n",
    "Мы установили, что гистограммы подчиняются мультиномиальному закону, где каждый бин $r_k$ - это компонента вектора наблюдения\n",
    "\n",
    "$$\n",
    "P(\\vec{r}) = \\frac{N!}{x_1!x_2!\\ldots x_K!} p_1^{x_1} p_2^{x_2} \\ldots p_K^{x_K}\n",
    "$$\n",
    "\n",
    "При этом несложно показать, что, как и биномиальное, мультиномиальное распределение сходится к распределению Пуассона в каждом бине.\n",
    "\n",
    "Далее, будем обозначать $\\mathcal{P}(x|\\lambda(\\vec{\\theta}))$ как распределение Пуассона. В итоге правдоподобие для гистограммы примет вид\n",
    "\n",
    "$$\n",
    "L = \\prod_k \\mathcal{P}(r_k | \\lambda_k(\\vec{\\theta}))\n",
    "\\\\\n",
    "\\ln L = - \\sum_k \\ln r_k! - \\sum_k \\lambda_k(\\vec{\\theta}) + \\sum_k r_k \\ln \\lambda_k(\\vec{\\theta})\n",
    "\\\\\n",
    "\\lambda_k = \\mu s_k + b_k\n",
    "$$\n",
    "\n",
    "При этом мы нередко можем заменить $\\lambda_k$ на непрерывную функцию $\\lambda(x, \\vec{\\theta})$ по $x_k$ (центрам бинов) или комбинацией гистограмм, полученных как пример на разных Монте-Карло генераторах.\n",
    "\n",
    "\n",
    "При достаточно большом числе наблюдений, мы ранее получили, что имеет место \n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum_k \\frac{(r_k - \\lambda_k(\\vec{\\theta}))^2}{\\lambda_k(\\vec{\\theta})}\n",
    "$$\n",
    "\n",
    "Таким образом фит в случае гистограмм сводится к минимизации критерия $\\chi^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Вспомогательные параметры в частотном подходе\n",
    "\n",
    "При проведении эксперимента нам порой важно только измерение какой-то конкретной величины. При этом, во время построения полной функции правдоподобия оказывается она может зависеть от множества параметров, измерять которые нам не очень интересно. В случае Байесова подхода, мы могли бы просто проинтегрировать по ним, но частотном подходе - это становится довольно сложно сделать. В самом лучшем случае, мы можем построить такую статистику, которая не будет зависеть от вспомогательных параметров и с помощью нее определить область покрытия значений для экспериментальных наблюдений. В худшем случае придется использовать __profile likehood__.\n",
    "\n",
    "Для уменьшения влияния вспомогательных параметров на ширину доверительного интервала можно использовать дополнительные измерения $\\mathbf{Y}$, которые зависят только от вспомогательных параметров, что позволяет построить функцию правдоподобия в виде \n",
    "\n",
    "$$\n",
    "L(\\vec{\\theta}, \\vec{\\eta}|\\mathbf{X}, \\mathbf{Y}) = L(\\vec{\\theta},\\vec{\\eta}|\\mathbf{X}) L(\\vec{\\eta}|\\mathbf{Y}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Критерий Неймана-Пирсона\n",
    "\n",
    "В случае простых гипотез $H_0$ и $H_1$, как в примере выше, существует всюду оптимальный односторонний критерий\n",
    "\n",
    "$$\n",
    "\\lambda(\\mathbf{X}) = \\frac{L(H_1 | \\mathbf{X})}{L(H_0 | \\mathbf{X})} > k_{\\alpha}\n",
    "$$\n",
    "\n",
    "где $L$ - это функция правдоподобия для соответсвующей гипотезы, $k_{\\alpha}$ - специальная константа, которая определяется исходя из уровня значимости.\n",
    "\n",
    "Если $\\lambda(\\mathbf{X}) > k_{\\alpha}$, то принимают гипотезу $H_1$, если $\\lambda(\\mathbf{X}) \\le k_{\\alpha}$ - $H_0$.\n",
    "\n",
    "В самом общем случае не всегда возможно построить функцию правдоподобия, в этом случае могут помочь методы машинного обучения, которые позволяют аппроксимировать это отношение.\n",
    "\n",
    "Также не всегда возможно найти распределение $\\lambda(\\mathbf{X})$ в явном виде. В этом случае, данное распределение можно получить с помощью Монте-Карло генераторов. То есть мы банально генерируем как можно больше вариантов исхода, для которых смотрим значение критерия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile Likehood\n",
    "\n",
    "Одним из интересных подходов к учету вспомогателных параметров является - profile likehood. В этом подходе фиксируются интересуемыми параметры, а остальные параметры оцениваются исходя из этого методом максимального правдоподобия\n",
    "\n",
    "$$\n",
    "L(\\vec{\\theta}) = \\max_{\\vec{\\omega}} L(\\vec{\\theta}, \\vec{\\omega}) = L(\\vec{\\theta}, \\hat{\\vec{\\omega}})\n",
    "$$\n",
    "\n",
    "Здесь $\\hat{\\vec{\\omega}}$ - это оценка по методу максимального правдоподобия при заданых каких-то значениях $\\vec{\\theta}$.\n",
    "\n",
    "Рассмотрим для примера такую функцию правдоподобия, в которой $\\sigma^2$ является вспомогательным параметром\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{(2\\pi\\sigma^2)^{N/2}} \\exp \\left(\n",
    "-\\frac{1}{2\\sigma^2} \\sum_n (x_n - \\mu)^2\n",
    "\\right)\n",
    "\\\\\n",
    "\\ln L = - \\frac{N}{2} \\ln \\sigma^2 - \\frac{1}{2\\sigma^2} \\sum_n (x_n - \\mu)^2\n",
    "$$\n",
    "\n",
    "Откуда \n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2(\\mu) = \\frac{1}{N} \\sum_n (x_n - \\mu)^2 \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теорема Вилка (Wilk's theorem)\n",
    "\n",
    "Пусть у нас есть некая функция распределения $f(\\mathbf{X} | \\vec{\\theta})$ (функция правдоподобия), котороая зависит от $\\vec{\\theta}$ параметров.\n",
    "\n",
    "Пусть у нас есть гипотеза $H_0$, такая что $\\vec{\\theta} \\in \\Theta_0$, где $\\Theta_0$ - это некая область в пространстве возможных значений набора параметров (в оригинальной статье, Вилк просто фиксировал некоторые параметры, задавая для них значения).\n",
    "\n",
    "Пусть у нас есть альтернативная гипотеза, такая что $\\vec{\\theta} \\in \\Omega$, где $\\Omega$ - это пространство всех возможных значений параметра (ни один параметр не зафиксирован).\n",
    "\n",
    "\n",
    "Это нам дает критерий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\lambda = -2 \\ln \\frac{ \\max\\limits_{\\vec{\\theta} \\in \\Theta_0} f(\\mathbf{X}, \\vec{\\theta}) }\n",
    "{ \\max\\limits_{\\vec{\\theta} \\in \\Omega} f(\\mathbf{X}, \\vec{\\theta}) }\n",
    "$$\n",
    "\n",
    "Здесь видно, что гипотеза $H_0$ включена в общее пространство значений $\\Omega$.\n",
    "\n",
    "> Если $\\nu$ - размерность пространства $\\Omega$, а $r$ - размерность $\\Theta_0$, то величина $\\lambda$ в предположении правдивости гипотезы $H_0$ распределена асимптотически как $\\chi^2_{\\nu - r}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это очень удобно использовать с профилем функции правдоподобия\n",
    "\n",
    "$$\n",
    "\\lambda(\\vec{\\theta}) = -2 \\ln \\frac{\n",
    "    L(\\vec{\\theta}, \\hat{\\vec{\\omega}} | \\mathbf{X})\n",
    "}{\n",
    "    L(\\hat{\\vec{\\theta}}, \\hat{\\hat{\\vec{\\omega}}} | \\mathbf{X})\n",
    "}\n",
    "$$\n",
    "\n",
    "где $\\hat{\\vec{\\omega}}$ оценка при заданном значении $\\vec{\\theta}$, $\\hat{\\hat{\\vec{\\omega}}}$ - оценка полученная вместе с оценкой $\\hat{\\vec{\\theta}}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
